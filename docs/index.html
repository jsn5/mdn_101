<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mixture Density Networks</title>
    <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,300;0,400;0,500;0,600;1,400&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}]});"></script>
    <style>
        :root {
            --bg: #faf9f7;
            --text: #1a1a1a;
            --text-muted: #666;
            --accent: #c45d4a;
            --accent-light: #f4ebe8;
            --border: #e5e3df;
            --code-bg: #f5f4f2;
            --success: #2d7d46;
            --blue: #4a7fb5;
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Crimson Pro', Georgia, serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.75;
            font-size: 19px;
        }
        .container { max-width: 760px; margin: 0 auto; padding: 80px 24px 120px; }
        header { margin-bottom: 72px; padding-bottom: 48px; border-bottom: 1px solid var(--border); }
        .tag {
            font-family: 'JetBrains Mono', monospace;
            font-size: 11px;
            letter-spacing: 0.1em;
            text-transform: uppercase;
            color: var(--accent);
            margin-bottom: 16px;
            display: block;
        }
        h1 { font-size: 42px; font-weight: 400; line-height: 1.2; letter-spacing: -0.02em; margin-bottom: 24px; }
        .subtitle { font-size: 21px; color: var(--text-muted); font-style: italic; font-weight: 300; }
        h2 { font-size: 28px; font-weight: 500; margin: 64px 0 24px; letter-spacing: -0.01em; }
        h3 { font-size: 22px; font-weight: 500; margin: 48px 0 16px; }
        p { margin-bottom: 24px; }
        .highlight { background: var(--accent-light); padding: 2px 6px; border-radius: 3px; }
        
        /* Elegant math blocks */
        .equation-block {
            background: linear-gradient(135deg, #fefefe 0%, var(--code-bg) 100%);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 32px 36px;
            margin: 36px 0;
            text-align: center;
            box-shadow: 0 2px 8px rgba(0,0,0,0.04);
        }
        .equation-block .katex { font-size: 1.3em; }
        .equation-block.hero {
            background: linear-gradient(135deg, var(--accent-light) 0%, #fff 100%);
            border-color: var(--accent);
            padding: 40px;
        }
        .equation-block.hero .katex { font-size: 1.5em; }
        
        .equation-label {
            font-family: 'JetBrains Mono', monospace;
            font-size: 11px;
            text-transform: uppercase;
            letter-spacing: 0.1em;
            color: var(--accent);
            margin-bottom: 16px;
            display: block;
        }
        
        .equation-explanation {
            margin-top: 20px;
            padding-top: 20px;
            border-top: 1px solid var(--border);
            text-align: left;
            font-size: 16px;
            color: var(--text-muted);
            line-height: 1.7;
        }
        .equation-explanation strong { color: var(--text); }
        
        /* Parameter cards */
        .param-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 16px;
            margin: 24px 0;
        }
        .param-card {
            background: white;
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 20px;
            text-align: center;
        }
        .param-card .katex { font-size: 1.2em; margin-bottom: 8px; }
        .param-card .param-name {
            font-family: 'JetBrains Mono', monospace;
            font-size: 12px;
            color: var(--accent);
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-bottom: 4px;
        }
        .param-card .param-desc {
            font-size: 14px;
            color: var(--text-muted);
        }
        
        /* Step-by-step math */
        .math-steps {
            background: var(--code-bg);
            border-radius: 8px;
            padding: 24px 28px;
            margin: 32px 0;
        }
        .math-step {
            display: flex;
            align-items: center;
            gap: 16px;
            padding: 12px 0;
            border-bottom: 1px solid var(--border);
        }
        .math-step:last-child { border-bottom: none; }
        .step-num {
            width: 28px;
            height: 28px;
            background: var(--accent);
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-family: 'JetBrains Mono', monospace;
            font-size: 13px;
            font-weight: 500;
            flex-shrink: 0;
        }
        .step-content { flex: 1; }
        .step-content .katex { font-size: 1.1em; }
        .step-desc {
            font-size: 15px;
            color: var(--text-muted);
            margin-top: 4px;
        }
        
        /* Code blocks for math breakdown */
        .code-breakdown {
            background: #fafafa;
            border: 1px solid var(--border);
            border-radius: 8px;
            margin: 32px 0;
            overflow: hidden;
        }
        .code-breakdown-header {
            background: #f5f4f2;
            padding: 10px 20px;
            border-bottom: 1px solid var(--border);
            display: flex;
            align-items: center;
            justify-content: space-between;
        }
        .code-breakdown-header .title {
            font-family: 'JetBrains Mono', monospace;
            font-size: 12px;
            font-weight: 500;
            color: var(--text-muted);
            letter-spacing: 0.02em;
        }
        .code-breakdown-header .lang {
            font-family: 'JetBrains Mono', monospace;
            font-size: 10px;
            color: var(--accent);
            text-transform: uppercase;
            letter-spacing: 0.05em;
            background: var(--accent-light);
            padding: 3px 8px;
            border-radius: 4px;
        }
        .code-breakdown pre {
            margin: 0;
            padding: 24px 24px 28px;
            overflow-x: auto;
            font-family: 'JetBrains Mono', monospace;
            font-size: 13px;
            line-height: 2;
            color: #37352f;
            background: transparent;
        }
        .code-breakdown .comment { color: #6b7280; font-style: italic; }
        .code-breakdown .keyword { color: #9333ea; }
        .code-breakdown .function { color: var(--accent); }
        .code-breakdown .number { color: #0891b2; }
        .code-breakdown .string { color: #16a34a; }
        .code-breakdown .variable { color: #1e40af; }
        .code-breakdown .operator { color: #37352f; }
        .code-breakdown .highlight-line {
            background: linear-gradient(90deg, var(--accent-light) 0%, rgba(244,235,232,0) 100%);
            display: block;
            margin: 4px -24px;
            padding: 4px 24px;
            border-left: 3px solid var(--accent);
        }
        
        /* Math to code transition */
        .math-to-code {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 16px;
            margin: 24px 0;
            font-family: 'JetBrains Mono', monospace;
            font-size: 13px;
            color: var(--text-muted);
        }
        .math-to-code .arrow {
            font-size: 24px;
            color: var(--accent);
        }
        
        .visualization {
            background: white;
            border: 1px solid var(--border);
            border-radius: 8px;
            margin: 40px 0;
            overflow: hidden;
        }
        .viz-header {
            padding: 16px 24px;
            border-bottom: 1px solid var(--border);
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 12px;
        }
        .viz-title {
            font-family: 'JetBrains Mono', monospace;
            font-size: 13px;
            font-weight: 500;
            color: var(--text-muted);
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }
        .viz-canvas-container { padding: 24px; display: flex; justify-content: center; background: #fefefe; }
        canvas { display: block; }
        .viz-controls {
            padding: 16px 24px;
            border-top: 1px solid var(--border);
            display: flex;
            gap: 12px;
            flex-wrap: wrap;
            align-items: center;
        }
        .viz-stats {
            padding: 12px 24px;
            border-top: 1px solid var(--border);
            background: var(--code-bg);
            font-family: 'JetBrains Mono', monospace;
            font-size: 12px;
            color: var(--text-muted);
            display: flex;
            gap: 24px;
            flex-wrap: wrap;
        }
        .stat-item { display: flex; gap: 8px; }
        .stat-label { color: var(--text-muted); }
        .stat-value { color: var(--text); font-weight: 500; }
        button {
            font-family: 'JetBrains Mono', monospace;
            font-size: 12px;
            padding: 10px 20px;
            background: var(--text);
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            transition: all 0.2s ease;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }
        button:hover { background: var(--accent); }
        button:disabled { background: #ccc; cursor: not-allowed; }
        button.secondary { background: transparent; color: var(--text); border: 1px solid var(--border); }
        button.secondary:hover { border-color: var(--accent); color: var(--accent); }
        button.success { background: var(--success); }
        button.success:hover { background: #246b3a; }
        .legend {
            display: flex;
            gap: 16px;
            font-family: 'JetBrains Mono', monospace;
            font-size: 11px;
            color: var(--text-muted);
            flex-wrap: wrap;
        }
        .legend-item { display: flex; align-items: center; gap: 6px; }
        .legend-dot { width: 10px; height: 10px; border-radius: 50%; }
        .legend-line { width: 16px; height: 3px; border-radius: 1px; }
        .callout { background: var(--accent-light); border-radius: 6px; padding: 24px 28px; margin: 32px 0; }
        .callout-title { font-weight: 600; margin-bottom: 8px; color: var(--accent); }
        .callout.success { background: #e8f5e9; }
        .callout.success .callout-title { color: var(--success); }
        .callout.blue { background: #e3f2fd; }
        .callout.blue .callout-title { color: var(--blue); }
        ul { margin: 16px 0 24px 24px; }
        li { margin-bottom: 8px; }
        .architecture {
            font-family: 'JetBrains Mono', monospace;
            font-size: 13px;
            background: var(--code-bg);
            padding: 20px 24px;
            border-radius: 6px;
            margin: 24px 0;
            text-align: center;
            line-height: 2.2;
        }
        .arch-layer {
            display: inline-block;
            padding: 4px 12px;
            background: white;
            border: 1px solid var(--border);
            border-radius: 4px;
            margin: 4px;
        }
        .arch-arrow { color: var(--accent); margin: 0 4px; }
        .slider-container { display: flex; align-items: center; gap: 12px; }
        .slider-label { font-family: 'JetBrains Mono', monospace; font-size: 11px; color: var(--text-muted); min-width: 60px; }
        input[type="range"] {
            width: 100px;
            height: 4px;
            -webkit-appearance: none;
            background: var(--border);
            border-radius: 2px;
            outline: none;
        }
        input[type="range"]::-webkit-slider-thumb {
            -webkit-appearance: none;
            width: 14px;
            height: 14px;
            background: var(--accent);
            border-radius: 50%;
            cursor: pointer;
        }
        .parameter-display {
            font-family: 'JetBrains Mono', monospace;
            font-size: 12px;
            color: var(--text);
            padding: 4px 10px;
            background: var(--code-bg);
            border-radius: 4px;
            min-width: 50px;
            text-align: center;
        }
        footer { margin-top: 80px; padding-top: 32px; border-top: 1px solid var(--border); font-size: 15px; color: var(--text-muted); }
        .divider { width: 40px; height: 2px; background: var(--accent); margin: 48px 0; }
        .progress-bar { width: 100%; height: 4px; background: var(--border); border-radius: 2px; overflow: hidden; }
        .progress-fill { height: 100%; background: var(--accent); transition: width 0.1s ease; }
        .step-indicator {
            font-family: 'JetBrains Mono', monospace;
            font-size: 12px;
            color: var(--accent);
            margin-bottom: 8px;
            text-transform: uppercase;
            letter-spacing: 0.1em;
        }
        @media (max-width: 600px) {
            body { font-size: 17px; }
            h1 { font-size: 32px; }
            h2 { font-size: 24px; }
            .container { padding: 48px 20px 80px; }
            .viz-stats { flex-direction: column; gap: 8px; }
            .param-grid { grid-template-columns: 1fr; }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <span class="tag">Deep Learning · Uncertainty · Code-First</span>
            <h1>Mixture Density Networks</h1>
            <p class="subtitle">Why standard regression collapses on inverse problems</p>
        </header>

        <article>
            <p>Standard neural networks predict a single output for a given input. That assumption is rarely questioned until you try to invert a function.</p>
            
            <p>This tutorial shows exactly where and why that assumption breaks, and how Mixture Density Networks (MDNs) fix it by modeling distributions, not point estimates.</p>

            <h2>Part 1: The Forward Problem</h2>
            <p class="step-indicator">Everything works</p>
            
            <p>Start with a function that is slightly nonlinear but well-behaved:</p>

            <div class="equation-block">
                <span class="equation-label">Forward Function</span>
                $$y = x + 0.3 \sin(2\pi x) + \epsilon$$
                <div class="equation-explanation">
                    where $\epsilon \sim \text{Uniform}(-0.1, 0.1)$ is small random noise.
                </div>
            </div>

            <p>For each input $x$, there is essentially one valid output $y$, up to noise. That means $p(y|x)$ is <strong>unimodal</strong>, the conditional mean $\mathbb{E}[y|x]$ is meaningful, and mean-squared error (MSE) is a valid objective.</p>

            <div class="visualization">
                <div class="viz-header">
                    <span class="viz-title">Forward Problem: y = f(x)</span>
                    <div class="legend">
                        <div class="legend-item"><div class="legend-dot" style="background: var(--success);"></div><span>Data</span></div>
                        <div class="legend-item"><div class="legend-line" style="background: var(--accent);"></div><span>MLP prediction</span></div>
                    </div>
                </div>
                <div class="viz-canvas-container"><canvas id="forwardCanvas" width="660" height="300"></canvas></div>
                <div class="viz-stats">
                    <div class="stat-item"><span class="stat-label">Epoch:</span><span class="stat-value" id="forwardEpoch">0</span></div>
                    <div class="stat-item"><span class="stat-label">MSE Loss:</span><span class="stat-value" id="forwardLoss">-</span></div>
                    <div class="stat-item"><span class="stat-label">Status:</span><span class="stat-value" id="forwardStatus">Ready</span></div>
                </div>
                <div class="viz-controls">
                    <button id="trainForward" class="success">Train MLP</button>
                    <button id="resetForward" class="secondary">Reset</button>
                    <div class="progress-bar" style="flex: 1; max-width: 200px;"><div class="progress-fill" id="forwardProgress" style="width: 0%;"></div></div>
                </div>
            </div>

            <div class="callout success">
                <div class="callout-title">Result</div>
                <p style="margin-bottom: 0;">A plain MLP trained with MSE converges cleanly. Nothing surprising. The network learns the conditional mean $\mathbb{E}[y|x]$, which happens to coincide with the function here.</p>
            </div>

            <div class="divider"></div>

            <h2>Part 2: The Inverse Problem</h2>
            <p class="step-indicator">Where it breaks</p>

            <p>Now swap the axes. Same data. Same points. Different question:</p>

            <div class="equation-block">
                <span class="equation-label">The Inversion</span>
                $$\text{Forward: } y = f(x) \quad \longrightarrow \quad \text{Inverse: } x = f^{-1}(y)$$
                <div class="equation-explanation">
                    Given $y$, what was $x$? This is no longer a well-posed regression problem.
                </div>
            </div>

            <p>Because the forward function is non-monotonic, multiple $x$ values map to the same $y$. So now $p(x|y)$ is <strong>multi-modal</strong>, there is no single correct answer, and the conditional mean lies between valid solutions.</p>

            <div class="visualization">
                <div class="viz-header">
                    <span class="viz-title">Inverse Problem: x = f⁻¹(y)</span>
                    <div class="legend">
                        <div class="legend-item"><div class="legend-dot" style="background: rgba(26,26,26,0.5);"></div><span>Data</span></div>
                        <div class="legend-item"><div class="legend-line" style="background: var(--accent);"></div><span>MLP prediction</span></div>
                    </div>
                </div>
                <div class="viz-canvas-container"><canvas id="inverseCanvas" width="660" height="300"></canvas></div>
                <div class="viz-stats">
                    <div class="stat-item"><span class="stat-label">Epoch:</span><span class="stat-value" id="inverseEpoch">0</span></div>
                    <div class="stat-item"><span class="stat-label">MSE Loss:</span><span class="stat-value" id="inverseLoss">-</span></div>
                    <div class="stat-item"><span class="stat-label">Status:</span><span class="stat-value" id="inverseStatus">Ready</span></div>
                </div>
                <div class="viz-controls">
                    <button id="trainInverse">Train MLP</button>
                    <button id="resetInverse" class="secondary">Reset</button>
                    <div class="progress-bar" style="flex: 1; max-width: 200px;"><div class="progress-fill" id="inverseProgress" style="width: 0%;"></div></div>
                </div>
            </div>

            <div class="callout">
                <div class="callout-title">Result</div>
                <p style="margin-bottom: 0;">The MLP produces $\hat{x}(y) = \mathbb{E}[x|y]$, a curve that passes through empty space. This is not underfitting. This is not lack of capacity. This is a <strong>loss-model mismatch</strong>.</p>
            </div>

            <h3>Why This Is a Fundamental Failure</h3>
            
            <p>MSE implicitly assumes: one correct target per input, symmetric penalties around that target, and a unimodal conditional distribution. Inverse problems violate all three.</p>

            <div class="callout blue">
                <div class="callout-title">This shows up everywhere</div>
                <p style="margin-bottom: 0;">Inverse kinematics (multiple joint configurations), vision (depth ambiguity), physics (cause from effect), control and planning. If your data has multiple valid outputs, point regression is the wrong abstraction.</p>
            </div>

            <div class="divider"></div>

            <h2>Part 3: What MDNs Change</h2>
            <p class="step-indicator">And what they don't</p>

            <p>Mixture Density Networks do not make the network more powerful. They change what the network is allowed to represent. Instead of predicting a value, the network predicts parameters of a conditional distribution.</p>

            <h3>The Formula</h3>
            
            <p>This is what you'll see in papers:</p>
            
            <div class="equation-block hero">
                <span class="equation-label">Gaussian Mixture Model</span>
                $$p(x \mid y) = \sum_{k=1}^{K} \pi_k(y) \cdot \mathcal{N}\big(x \mid \mu_k(y), \sigma_k(y)\big)$$
            </div>

            <p>Each input $y$ produces K candidate solutions. Each solution has a probability mass. The model is free to represent multiple modes. This is just a Gaussian mixture whose parameters are functions of the input.</p>
            
            <p>The summation symbol scares people unnecessarily. It's just a loop:</p>

            <div class="code-breakdown">
                <div class="code-breakdown-header"><span class="title">gmm_probability.py</span><span class="lang">python</span></div>
<pre><span class="keyword">def</span> <span class="function">gmm_probability</span>(<span class="variable">x</span>, <span class="variable">y</span>, <span class="variable">network</span>):
    <span class="variable">pi</span>, <span class="variable">mu</span>, <span class="variable">sigma</span> = <span class="variable">network</span>.<span class="function">forward</span>(<span class="variable">y</span>)   <span class="comment"># Get mixture params from network</span>
<span class="highlight-line">    <span class="variable">total</span> = <span class="number">0</span></span>
<span class="highlight-line">    <span class="keyword">for</span> <span class="variable">k</span> <span class="keyword">in</span> <span class="function">range</span>(<span class="variable">K</span>):                      <span class="comment"># Σ is just a for loop</span></span>
<span class="highlight-line">        <span class="variable">total</span> += <span class="variable">pi</span>[<span class="variable">k</span>] * <span class="function">normal_pdf</span>(<span class="variable">x</span>, <span class="variable">mu</span>[<span class="variable">k</span>], <span class="variable">sigma</span>[<span class="variable">k</span>])</span>
    <span class="keyword">return</span> <span class="variable">total</span></pre>
            </div>

            <h3>The Gaussian (Bell Curve)</h3>
            <p>The $\mathcal{N}(x \mid \mu, \sigma)$ term is the standard normal density:</p>

            <div class="code-breakdown">
                <div class="code-breakdown-header"><span class="title">gaussian.py</span><span class="lang">python</span></div>
<pre><span class="keyword">def</span> <span class="function">normal_pdf</span>(<span class="variable">x</span>, <span class="variable">mu</span>, <span class="variable">sigma</span>):
    <span class="variable">z</span> = (<span class="variable">x</span> - <span class="variable">mu</span>) / <span class="variable">sigma</span>
    <span class="keyword">return</span> <span class="function">exp</span>(-<span class="number">0.5</span> * <span class="variable">z</span>**<span class="number">2</span>) / (<span class="variable">sigma</span> * <span class="function">sqrt</span>(<span class="number">2</span> * <span class="variable">pi</span>))</pre>
            </div>

            <h3>Output Transformations</h3>
            <p>The network outputs raw numbers. We transform them into valid probability parameters:</p>

            <div class="code-breakdown">
                <div class="code-breakdown-header"><span class="title">transform_outputs.py</span><span class="lang">python</span></div>
<pre><span class="keyword">def</span> <span class="function">transform_outputs</span>(<span class="variable">raw</span>):
    <span class="variable">z_pi</span>, <span class="variable">z_mu</span>, <span class="variable">z_sigma</span> = <span class="variable">raw</span>[<span class="number">0</span>:<span class="number">3</span>], <span class="variable">raw</span>[<span class="number">3</span>:<span class="number">6</span>], <span class="variable">raw</span>[<span class="number">6</span>:<span class="number">9</span>]
<span class="highlight-line">    <span class="variable">pi</span>    = <span class="function">softmax</span>(<span class="variable">z_pi</span>)    <span class="comment"># Must sum to 1</span></span>
<span class="highlight-line">    <span class="variable">mu</span>    = <span class="variable">z_mu</span>              <span class="comment"># Unconstrained</span></span>
<span class="highlight-line">    <span class="variable">sigma</span> = <span class="function">exp</span>(<span class="variable">z_sigma</span>)     <span class="comment"># Must be > 0</span></span>
    <span class="keyword">return</span> <span class="variable">pi</span>, <span class="variable">mu</span>, <span class="variable">sigma</span></pre>
            </div>

            <h3>Why the Loss Function Matters More Than the Architecture</h3>

            <p>MDNs are trained with negative log-likelihood:</p>
            
            <div class="equation-block">
                <span class="equation-label">Loss Function</span>
                $$\mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} \log \left( \sum_{k=1}^{K} \pi_k \cdot \mathcal{N}(x_i \mid \mu_k, \sigma_k) \right)$$
            </div>
            
            <p>Key difference from MSE: MSE penalizes distance from all targets. NLL rewards being close to any valid mode. So if your data has three branches, the model learns three Gaussians, not their average. This is the core fix.</p>

            <div class="code-breakdown">
                <div class="code-breakdown-header"><span class="title">loss.py</span><span class="lang">python</span></div>
<pre><span class="keyword">def</span> <span class="function">mdn_loss</span>(<span class="variable">x_data</span>, <span class="variable">y_data</span>, <span class="variable">network</span>):
    <span class="variable">total_loss</span> = <span class="number">0</span>
<span class="highlight-line">    <span class="keyword">for</span> <span class="variable">i</span> <span class="keyword">in</span> <span class="function">range</span>(<span class="variable">N</span>):</span>
<span class="highlight-line">        <span class="variable">prob</span> = <span class="function">gmm_probability</span>(<span class="variable">x_data</span>[<span class="variable">i</span>], <span class="variable">y_data</span>[<span class="variable">i</span>], <span class="variable">network</span>)</span>
<span class="highlight-line">        <span class="variable">total_loss</span> += -<span class="function">log</span>(<span class="variable">prob</span>)  <span class="comment"># Negative log probability</span></span>
    <span class="keyword">return</span> <span class="variable">total_loss</span> / <span class="variable">N</span></pre>
            </div>

            <div class="callout blue">
                <div class="callout-title">The Core Fix</div>
                <p style="margin-bottom: 0;">MSE punishes you for being far from all targets. NLL only cares that some Gaussian covers each point. If your data has 3 branches, the network learns to put one Gaussian on each branch, not their average.</p>
            </div>

            <div class="visualization">
                <div class="viz-header">
                    <span class="viz-title">MDN on Inverse Problem (K=3)</span>
                    <div class="legend">
                        <div class="legend-item"><div class="legend-dot" style="background: rgba(26,26,26,0.4);"></div><span>Data</span></div>
                        <div class="legend-item"><div class="legend-dot" style="background: rgba(196,93,74,0.5);"></div><span>Density</span></div>
                        <div class="legend-item"><div class="legend-dot" style="background: var(--success);"></div><span>Samples</span></div>
                    </div>
                </div>
                <div class="viz-canvas-container"><canvas id="mdnCanvas" width="660" height="300"></canvas></div>
                <div class="viz-stats">
                    <div class="stat-item"><span class="stat-label">Epoch:</span><span class="stat-value" id="mdnEpoch">0</span></div>
                    <div class="stat-item"><span class="stat-label">NLL Loss:</span><span class="stat-value" id="mdnLoss">-</span></div>
                    <div class="stat-item"><span class="stat-label">Status:</span><span class="stat-value" id="mdnStatus">Ready</span></div>
                </div>
                <div class="viz-controls">
                    <button id="trainMDN" class="success">Train MDN</button>
                    <button id="sampleMDN" class="secondary">Sample</button>
                    <button id="resetMDN" class="secondary">Reset</button>
                    <div class="progress-bar" style="flex: 1; max-width: 200px;"><div class="progress-fill" id="mdnProgress" style="width: 0%;"></div></div>
                </div>
            </div>

            <div class="callout success">
                <div class="callout-title">Result</div>
                <p style="margin-bottom: 0;">Each Gaussian locks onto a different inverse branch. Mixing weights adapt smoothly across input space. Sampling produces valid solutions, not averages. The model is no longer forced to lie.</p>
            </div>

            <h3>Sampling Is Not an Afterthought</h3>
            <p>MDNs give you a distribution, not a number. How you consume that distribution is a downstream choice:</p>

            <div class="code-breakdown">
                <div class="code-breakdown-header"><span class="title">sample.py</span><span class="lang">python</span></div>
<pre><span class="keyword">def</span> <span class="function">sample_from_mdn</span>(<span class="variable">y</span>, <span class="variable">network</span>):
    <span class="variable">pi</span>, <span class="variable">mu</span>, <span class="variable">sigma</span> = <span class="variable">network</span>.<span class="function">forward</span>(<span class="variable">y</span>)
<span class="highlight-line">    <span class="variable">k</span> = <span class="function">random_choice</span>([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], <span class="variable">weights</span>=<span class="variable">pi</span>)  <span class="comment"># Pick a component</span></span>
<span class="highlight-line">    <span class="keyword">return</span> <span class="function">random_normal</span>(<span class="variable">mu</span>[<span class="variable">k</span>], <span class="variable">sigma</span>[<span class="variable">k</span>])        <span class="comment"># Sample from it</span></span></pre>
            </div>

            <h2>Component Analysis</h2>
            <p>Move the slider to see how the mixture components adapt across input space. In the multi-modal region, multiple components are active. At the edges, one dominates.</p>

            <div class="visualization">
                <div class="viz-header">
                    <span class="viz-title">Mixture Components at Position y</span>
                    <div class="legend">
                        <div class="legend-item"><div class="legend-dot" style="background: #4a90e2;"></div><span>π₁</span></div>
                        <div class="legend-item"><div class="legend-dot" style="background: #50c878;"></div><span>π₂</span></div>
                        <div class="legend-item"><div class="legend-dot" style="background: #f5a623;"></div><span>π₃</span></div>
                        <div class="legend-item"><div class="legend-line" style="background: var(--accent);"></div><span>Mixture</span></div>
                    </div>
                </div>
                <div class="viz-canvas-container"><canvas id="componentCanvas" width="660" height="240"></canvas></div>
                <div class="viz-controls">
                    <div class="slider-container">
                        <span class="slider-label">Position:</span>
                        <input type="range" id="xSlider" min="5" max="95" value="50">
                        <span class="parameter-display" id="xValue">y = 0.50</span>
                    </div>
                </div>
            </div>

            <h2>The Takeaway</h2>
            
            <div class="code-breakdown">
                <div class="code-breakdown-header"><span class="title">tldr.py</span><span class="lang">python</span></div>
<pre><span class="comment"># Standard regression</span>
<span class="variable">y_pred</span> = <span class="variable">network</span>(<span class="variable">x</span>)              <span class="comment"># One output</span>
<span class="variable">loss</span>   = <span class="function">mean</span>((<span class="variable">y_pred</span> - <span class="variable">y</span>)**<span class="number">2</span>)   <span class="comment"># MSE: assumes one right answer</span>
<span class="comment"># MDN regression</span>
<span class="variable">pi</span>, <span class="variable">mu</span>, <span class="variable">sigma</span> = <span class="variable">network</span>(<span class="variable">x</span>)       <span class="comment"># Multiple Gaussians</span>
<span class="variable">loss</span> = -<span class="function">mean</span>(<span class="function">log</span>(<span class="function">gmm_prob</span>(<span class="variable">y</span>)))   <span class="comment"># NLL: allows many truths</span>
<span class="comment"># Use MDNs when one input → multiple valid outputs</span>
<span class="comment"># Do not use if unimodal. Adds complexity without benefit.</span></pre>
            </div>
        </article>

        <footer><p>Neural networks implemented in vanilla JavaScript. No frameworks. Inspired by Bishop's <em>Pattern Recognition and Machine Learning</em> and Mike Dusenberry's MDN tutorial.</p></footer>
    </div>

<script>
// ================================================================
// MATRIX CLASS
// ================================================================
class Matrix {
    constructor(rows, cols, data = null) {
        this.rows = rows;
        this.cols = cols;
        this.data = data || new Float64Array(rows * cols);
    }
    static zeros(rows, cols) { return new Matrix(rows, cols); }
    static random(rows, cols, scale = 1) {
        const m = new Matrix(rows, cols);
        for (let i = 0; i < m.data.length; i++) m.data[i] = (Math.random() * 2 - 1) * scale;
        return m;
    }
    get(i, j) { return this.data[i * this.cols + j]; }
    set(i, j, v) { this.data[i * this.cols + j] = v; }
    add(o) { const r = new Matrix(this.rows, this.cols); for (let i = 0; i < this.data.length; i++) r.data[i] = this.data[i] + o.data[i]; return r; }
    sub(o) { const r = new Matrix(this.rows, this.cols); for (let i = 0; i < this.data.length; i++) r.data[i] = this.data[i] - o.data[i]; return r; }
    scale(s) { const r = new Matrix(this.rows, this.cols); for (let i = 0; i < this.data.length; i++) r.data[i] = this.data[i] * s; return r; }
    static matmul(a, b) {
        const r = new Matrix(a.rows, b.cols);
        for (let i = 0; i < a.rows; i++)
            for (let j = 0; j < b.cols; j++) {
                let s = 0;
                for (let k = 0; k < a.cols; k++) s += a.get(i, k) * b.get(k, j);
                r.set(i, j, s);
            }
        return r;
    }
    static transpose(m) {
        const r = new Matrix(m.cols, m.rows);
        for (let i = 0; i < m.rows; i++) for (let j = 0; j < m.cols; j++) r.set(j, i, m.get(i, j));
        return r;
    }
    addBias(b) {
        const r = new Matrix(this.rows, this.cols);
        for (let i = 0; i < this.rows; i++) for (let j = 0; j < this.cols; j++) r.set(i, j, this.get(i, j) + b.get(0, j));
        return r;
    }
}

// ================================================================
// MLP with tanh + input/output normalization
// ================================================================
class MLP {
    constructor(inp, hid, out) {
        const s1 = Math.sqrt(6.0 / (inp + hid)) * 2;
        const s2 = Math.sqrt(6.0 / (hid + hid));
        const s3 = Math.sqrt(6.0 / (hid + out));
        
        this.W1 = Matrix.random(inp, hid, s1); this.b1 = Matrix.zeros(1, hid);
        this.W2 = Matrix.random(hid, hid, s2); this.b2 = Matrix.zeros(1, hid);
        this.W3 = Matrix.random(hid, out, s3); this.b3 = Matrix.zeros(1, out);
        
        for (let j = 0; j < hid; j++) this.b1.set(0, j, (Math.random() - 0.5) * 2);
        
        // Normalization params (set via setNorm)
        this.xMean = 0; this.xStd = 1;
        this.yMean = 0; this.yStd = 1;
    }
    
    setNorm(xMean, xStd, yMean, yStd) {
        this.xMean = xMean; this.xStd = xStd;
        this.yMean = yMean; this.yStd = yStd;
    }
    
    tanh(m) { const r = new Matrix(m.rows, m.cols); for (let i = 0; i < m.data.length; i++) r.data[i] = Math.tanh(m.data[i]); return r; }
    tanhD(m) { const r = new Matrix(m.rows, m.cols); for (let i = 0; i < m.data.length; i++) { const t = Math.tanh(m.data[i]); r.data[i] = 1 - t * t; } return r; }
    
    forward(X) {
        // Normalize input
        this.X = new Matrix(X.rows, X.cols);
        for (let i = 0; i < X.rows; i++) {
            this.X.set(i, 0, (X.get(i, 0) - this.xMean) / this.xStd);
        }
        
        this.z1 = Matrix.matmul(this.X, this.W1).addBias(this.b1); this.a1 = this.tanh(this.z1);
        this.z2 = Matrix.matmul(this.a1, this.W2).addBias(this.b2); this.a2 = this.tanh(this.z2);
        this.z3 = Matrix.matmul(this.a2, this.W3).addBias(this.b3);
        return this.z3;
    }
    
    had(a, b) { const r = new Matrix(a.rows, a.cols); for (let i = 0; i < a.data.length; i++) r.data[i] = a.data[i] * b.data[i]; return r; }
    sumR(m) { const r = Matrix.zeros(1, m.cols); for (let i = 0; i < m.rows; i++) for (let j = 0; j < m.cols; j++) r.set(0, j, r.get(0, j) + m.get(i, j)); return r; }
    clip(m, mx) { let n = 0; for (let i = 0; i < m.data.length; i++) n += m.data[i] * m.data[i]; n = Math.sqrt(n); return n > mx ? m.scale(mx / n) : m; }
    
    backward(Y, lr) {
        // Normalize target
        const Yn = new Matrix(Y.rows, Y.cols);
        for (let i = 0; i < Y.rows; i++) {
            Yn.set(i, 0, (Y.get(i, 0) - this.yMean) / this.yStd);
        }
        
        const n = this.X.rows;
        const dz3 = this.z3.sub(Yn).scale(2 / n);
        const dW3 = Matrix.matmul(Matrix.transpose(this.a2), dz3), db3 = this.sumR(dz3);
        const da2 = Matrix.matmul(dz3, Matrix.transpose(this.W3));
        const dz2 = this.had(da2, this.tanhD(this.z2));
        const dW2 = Matrix.matmul(Matrix.transpose(this.a1), dz2), db2 = this.sumR(dz2);
        const da1 = Matrix.matmul(dz2, Matrix.transpose(this.W2));
        const dz1 = this.had(da1, this.tanhD(this.z1));
        const dW1 = Matrix.matmul(Matrix.transpose(this.X), dz1), db1 = this.sumR(dz1);
        
        // Gradient clipping
        this.W3 = this.W3.sub(this.clip(dW3, 5).scale(lr)); this.b3 = this.b3.sub(this.clip(db3, 5).scale(lr));
        this.W2 = this.W2.sub(this.clip(dW2, 5).scale(lr)); this.b2 = this.b2.sub(this.clip(db2, 5).scale(lr));
        this.W1 = this.W1.sub(this.clip(dW1, 5).scale(lr)); this.b1 = this.b1.sub(this.clip(db1, 5).scale(lr));
    }
    
    loss(Y) { 
        const Yn = new Matrix(Y.rows, Y.cols);
        for (let i = 0; i < Y.rows; i++) Yn.set(i, 0, (Y.get(i, 0) - this.yMean) / this.yStd);
        let l = 0; 
        for (let i = 0; i < Y.rows; i++) { const d = this.z3.get(i, 0) - Yn.get(i, 0); l += d * d; } 
        return l / Y.rows; 
    }
    
    predict(x) { 
        const X = new Matrix(1, 1); 
        X.set(0, 0, x); 
        this.forward(X); 
        // Denormalize output
        return this.z3.get(0, 0) * this.yStd + this.yMean; 
    }
}

// ================================================================
// MDN with tanh (K=3 components) + normalization
// ================================================================
class MDN {
    constructor(inp, hid, K) {
        this.K = K;
        const s1 = Math.sqrt(6.0 / (inp + hid)) * 2;
        const s2 = Math.sqrt(6.0 / (hid + hid));
        
        this.W1 = Matrix.random(inp, hid, s1); this.b1 = Matrix.zeros(1, hid);
        this.W2 = Matrix.random(hid, hid, s2); this.b2 = Matrix.zeros(1, hid);
        
        for (let j = 0; j < hid; j++) this.b1.set(0, j, (Math.random() - 0.5) * 2);
        
        this.W3 = Matrix.random(hid, 3 * K, 0.1); 
        this.b3 = Matrix.zeros(1, 3 * K);
        
        for (let k = 0; k < K; k++) {
            this.b3.set(0, K + k, -1 + 2 * k / (K - 1)); // mu init spread in normalized space
            this.b3.set(0, 2 * K + k, -1); // sigma init
        }
        
        // Normalization params
        this.xMean = 0; this.xStd = 1;
        this.yMean = 0; this.yStd = 1;
    }
    
    setNorm(xMean, xStd, yMean, yStd) {
        this.xMean = xMean; this.xStd = xStd;
        this.yMean = yMean; this.yStd = yStd;
    }
    
    tanh(m) { const r = new Matrix(m.rows, m.cols); for (let i = 0; i < m.data.length; i++) r.data[i] = Math.tanh(m.data[i]); return r; }
    tanhD(m) { const r = new Matrix(m.rows, m.cols); for (let i = 0; i < m.data.length; i++) { const t = Math.tanh(m.data[i]); r.data[i] = 1 - t * t; } return r; }
    
    forward(X) {
        // Normalize input
        this.X = new Matrix(X.rows, X.cols);
        for (let i = 0; i < X.rows; i++) {
            this.X.set(i, 0, (X.get(i, 0) - this.xMean) / this.xStd);
        }
        
        this.z1 = Matrix.matmul(this.X, this.W1).addBias(this.b1); this.a1 = this.tanh(this.z1);
        this.z2 = Matrix.matmul(this.a1, this.W2).addBias(this.b2); this.a2 = this.tanh(this.z2);
        this.z3 = Matrix.matmul(this.a2, this.W3).addBias(this.b3);
        this.parse();
        return { pi: this.pi, mu: this.mu, sigma: this.sigma };
    }
    
    parse() {
        const n = this.z3.rows;
        this.pi = new Matrix(n, this.K); this.mu = new Matrix(n, this.K);
        this.sigma = new Matrix(n, this.K); this.logSigma = new Matrix(n, this.K);
        for (let i = 0; i < n; i++) {
            let mx = -Infinity;
            for (let k = 0; k < this.K; k++) mx = Math.max(mx, this.z3.get(i, k));
            let sm = 0;
            for (let k = 0; k < this.K; k++) sm += Math.exp(this.z3.get(i, k) - mx);
            for (let k = 0; k < this.K; k++) this.pi.set(i, k, Math.exp(this.z3.get(i, k) - mx) / sm);
            for (let k = 0; k < this.K; k++) this.mu.set(i, k, this.z3.get(i, this.K + k));
            for (let k = 0; k < this.K; k++) {
                const ls = Math.max(-5, Math.min(2, this.z3.get(i, 2 * this.K + k)));
                this.logSigma.set(i, k, ls);
                this.sigma.set(i, k, Math.exp(ls));
            }
        }
    }
    
    gauss(y, mu, sig) { const z = (y - mu) / sig; return Math.exp(-0.5 * z * z) / (sig * Math.sqrt(2 * Math.PI)); }
    
    loss(Y) {
        let L = 0;
        for (let i = 0; i < Y.rows; i++) {
            // Normalize target
            const y = (Y.get(i, 0) - this.yMean) / this.yStd;
            let p = 0;
            for (let k = 0; k < this.K; k++) p += this.pi.get(i, k) * this.gauss(y, this.mu.get(i, k), this.sigma.get(i, k));
            L -= Math.log(Math.max(p, 1e-10));
        }
        return L / Y.rows;
    }
    
    had(a, b) { const r = new Matrix(a.rows, a.cols); for (let i = 0; i < a.data.length; i++) r.data[i] = a.data[i] * b.data[i]; return r; }
    sumR(m) { const r = Matrix.zeros(1, m.cols); for (let i = 0; i < m.rows; i++) for (let j = 0; j < m.cols; j++) r.set(0, j, r.get(0, j) + m.get(i, j)); return r; }
    
    backward(Y, lr) {
        const n = Y.rows;
        const dz3 = Matrix.zeros(n, 3 * this.K);
        for (let i = 0; i < n; i++) {
            // Normalize target
            const y = (Y.get(i, 0) - this.yMean) / this.yStd;
            const gam = new Float64Array(this.K);
            let tot = 0;
            for (let k = 0; k < this.K; k++) { gam[k] = this.pi.get(i, k) * this.gauss(y, this.mu.get(i, k), this.sigma.get(i, k)); tot += gam[k]; }
            for (let k = 0; k < this.K; k++) gam[k] /= Math.max(tot, 1e-10);
            for (let k = 0; k < this.K; k++) dz3.set(i, k, (this.pi.get(i, k) - gam[k]) / n);
            for (let k = 0; k < this.K; k++) {
                const mu_k = this.mu.get(i, k), sig_k = this.sigma.get(i, k), d = y - mu_k;
                dz3.set(i, this.K + k, -gam[k] * d / (sig_k * sig_k) / n);
                dz3.set(i, 2 * this.K + k, gam[k] * (1 - d * d / (sig_k * sig_k)) / n);
            }
        }
        const dW3 = Matrix.matmul(Matrix.transpose(this.a2), dz3), db3 = this.sumR(dz3);
        const da2 = Matrix.matmul(dz3, Matrix.transpose(this.W3));
        const dz2 = this.had(da2, this.tanhD(this.z2));
        const dW2 = Matrix.matmul(Matrix.transpose(this.a1), dz2), db2 = this.sumR(dz2);
        const da1 = Matrix.matmul(dz2, Matrix.transpose(this.W2));
        const dz1 = this.had(da1, this.tanhD(this.z1));
        const dW1 = Matrix.matmul(Matrix.transpose(this.X), dz1), db1 = this.sumR(dz1);
        
        const clip = (m, mx) => { let n = 0; for (let i = 0; i < m.data.length; i++) n += m.data[i] * m.data[i]; n = Math.sqrt(n); return n > mx ? m.scale(mx / n) : m; };
        this.W3 = this.W3.sub(clip(dW3, 5).scale(lr)); this.b3 = this.b3.sub(clip(db3, 5).scale(lr));
        this.W2 = this.W2.sub(clip(dW2, 5).scale(lr)); this.b2 = this.b2.sub(clip(db2, 5).scale(lr));
        this.W1 = this.W1.sub(clip(dW1, 5).scale(lr)); this.b1 = this.b1.sub(clip(db1, 5).scale(lr));
    }
    
    getParams(x) {
        const X = new Matrix(1, 1); X.set(0, 0, x); this.forward(X);
        const p = { pi: [], mu: [], sigma: [] };
        for (let k = 0; k < this.K; k++) { 
            p.pi.push(this.pi.get(0, k)); 
            // Denormalize mu and sigma
            p.mu.push(this.mu.get(0, k) * this.yStd + this.yMean); 
            p.sigma.push(this.sigma.get(0, k) * this.yStd); 
        }
        return p;
    }
    
    sample(x) {
        const p = this.getParams(x);
        let r = Math.random(), k = 0, cs = p.pi[0];
        while (r > cs && k < this.K - 1) { k++; cs += p.pi[k]; }
        const u1 = Math.random(), u2 = Math.random();
        const z = Math.sqrt(-2 * Math.log(u1)) * Math.cos(2 * Math.PI * u2);
        return p.mu[k] + p.sigma[k] * z;
    }
    
    mixPDF(y, p) { let s = 0; for (let k = 0; k < p.pi.length; k++) s += p.pi[k] * this.gauss(y, p.mu[k], p.sigma[k]); return s; }
}

// ================================================================
// DATA GENERATION: y = x + 0.3*sin(2πx) + noise
// ================================================================
function genForwardData(n = 500) {
    const d = [];
    for (let i = 0; i < n; i++) {
        const x = Math.random();
        const noise = (Math.random() - 0.5) * 0.2;
        const y = x + 0.3 * Math.sin(2 * Math.PI * x) + noise;
        d.push({ x, y });
    }
    return d;
}

function invertData(data) {
    return data.map(p => ({ x: p.y, y: p.x }));
}

function toMat(d) {
    const X = new Matrix(d.length, 1), Y = new Matrix(d.length, 1);
    for (let i = 0; i < d.length; i++) { X.set(i, 0, d[i].x); Y.set(i, 0, d[i].y); }
    return { X, Y };
}

// ================================================================
// DRAWING
// ================================================================
const PAD = 50;
function clear(ctx, c) { ctx.fillStyle = '#fefefe'; ctx.fillRect(0, 0, c.width, c.height); }
function axes(ctx, c, xLabel = 'x', yLabel = 'y') {
    ctx.strokeStyle = '#e5e3df'; ctx.lineWidth = 1;
    ctx.beginPath(); ctx.moveTo(PAD, c.height - PAD); ctx.lineTo(c.width - PAD, c.height - PAD); ctx.stroke();
    ctx.beginPath(); ctx.moveTo(PAD, PAD); ctx.lineTo(PAD, c.height - PAD); ctx.stroke();
    ctx.fillStyle = '#999'; ctx.font = '11px "JetBrains Mono", monospace';
    ctx.fillText(xLabel, c.width - PAD + 8, c.height - PAD + 4);
    ctx.fillText(yLabel, PAD - 4, PAD - 10);
    ctx.fillText('0', PAD - 12, c.height - PAD + 14);
    ctx.fillText('1', c.width - PAD - 3, c.height - PAD + 14);
    ctx.fillText('1', PAD - 12, PAD + 4);
}
function toC(x, y, c, xMin = 0, xMax = 1, yMin = 0, yMax = 1) {
    const w = c.width - 2 * PAD, h = c.height - 2 * PAD;
    const px = (x - xMin) / (xMax - xMin);
    const py = (y - yMin) / (yMax - yMin);
    return { cx: PAD + px * w, cy: c.height - PAD - py * h };
}
function drawPts(ctx, c, d, color = 'rgba(26,26,26,0.5)', sz = 3, xMin = 0, xMax = 1, yMin = 0, yMax = 1) {
    ctx.fillStyle = color;
    d.forEach(p => { 
        const { cx, cy } = toC(p.x, p.y, c, xMin, xMax, yMin, yMax); 
        ctx.beginPath(); ctx.arc(cx, cy, sz, 0, Math.PI * 2); ctx.fill(); 
    });
}

// ================================================================
// STATE
// ================================================================
let forwardData = genForwardData(500);
let inverseData = invertData(forwardData);

// Compute bounds for the data
function getBounds(data) {
    let xMin = Infinity, xMax = -Infinity, yMin = Infinity, yMax = -Infinity;
    data.forEach(p => {
        xMin = Math.min(xMin, p.x); xMax = Math.max(xMax, p.x);
        yMin = Math.min(yMin, p.y); yMax = Math.max(yMax, p.y);
    });
    // Add padding
    const xPad = (xMax - xMin) * 0.05, yPad = (yMax - yMin) * 0.05;
    return { xMin: xMin - xPad, xMax: xMax + xPad, yMin: yMin - yPad, yMax: yMax + yPad };
}

let forwardBounds = getBounds(forwardData);
let inverseBounds = getBounds(inverseData);

let forwardMLP = new MLP(1, 32, 1);
let inverseMLP = new MLP(1, 64, 1);
let mdn = new MDN(1, 64, 3);

let forwardTrained = false, inverseTrained = false, mdnTrained = false;
let mdnSamples = [];

// ================================================================
// CANVASES
// ================================================================
const forwardC = document.getElementById('forwardCanvas'), forwardCtx = forwardC.getContext('2d');
const inverseC = document.getElementById('inverseCanvas'), inverseCtx = inverseC.getContext('2d');
const mdnC = document.getElementById('mdnCanvas'), mdnCtx = mdnC.getContext('2d');
const compC = document.getElementById('componentCanvas'), compCtx = compC.getContext('2d');

function drawForward() {
    clear(forwardCtx, forwardC); axes(forwardCtx, forwardC, 'x', 'y');
    const b = forwardBounds;
    drawPts(forwardCtx, forwardC, forwardData, 'rgba(45, 125, 70, 0.5)', 3, b.xMin, b.xMax, b.yMin, b.yMax);
    if (forwardTrained) {
        forwardCtx.strokeStyle = '#c45d4a'; forwardCtx.lineWidth = 2.5; forwardCtx.beginPath();
        for (let px = 0; px <= 100; px++) {
            const x = b.xMin + (px / 100) * (b.xMax - b.xMin);
            const y = forwardMLP.predict(x);
            const { cx, cy } = toC(x, y, forwardC, b.xMin, b.xMax, b.yMin, b.yMax);
            px === 0 ? forwardCtx.moveTo(cx, cy) : forwardCtx.lineTo(cx, cy);
        }
        forwardCtx.stroke();
    }
}

function drawInverse() {
    clear(inverseCtx, inverseC); axes(inverseCtx, inverseC, 'y', 'x');
    const b = inverseBounds;
    drawPts(inverseCtx, inverseC, inverseData, 'rgba(26,26,26,0.4)', 3, b.xMin, b.xMax, b.yMin, b.yMax);
    if (inverseTrained) {
        inverseCtx.strokeStyle = '#c45d4a'; inverseCtx.lineWidth = 2.5; inverseCtx.beginPath();
        for (let px = 0; px <= 100; px++) {
            const x = b.xMin + (px / 100) * (b.xMax - b.xMin);
            const y = inverseMLP.predict(x);
            const { cx, cy } = toC(x, y, inverseC, b.xMin, b.xMax, b.yMin, b.yMax);
            px === 0 ? inverseCtx.moveTo(cx, cy) : inverseCtx.lineTo(cx, cy);
        }
        inverseCtx.stroke();
    }
}

function drawMDN() {
    clear(mdnCtx, mdnC); axes(mdnCtx, mdnC, 'y', 'x');
    const b = inverseBounds;
    if (mdnTrained) {
        const w = mdnC.width - 2 * PAD, h = mdnC.height - 2 * PAD;
        for (let px = 0; px < w; px += 3) {
            const x = b.xMin + (px / w) * (b.xMax - b.xMin);
            const p = mdn.getParams(x);
            for (let py = 0; py < h; py += 3) {
                const y = b.yMax - (py / h) * (b.yMax - b.yMin);
                const dens = mdn.mixPDF(y, p);
                const alpha = Math.min(0.7, dens * 1.2);
                if (alpha > 0.02) {
                    mdnCtx.fillStyle = `rgba(196,93,74,${alpha})`;
                    mdnCtx.fillRect(PAD + px, PAD + py, 3, 3);
                }
            }
        }
    }
    drawPts(mdnCtx, mdnC, inverseData, 'rgba(26,26,26,0.35)', 3, b.xMin, b.xMax, b.yMin, b.yMax);
    if (mdnSamples.length) {
        mdnCtx.fillStyle = '#2d7d46';
        mdnSamples.forEach(s => {
            const { cx, cy } = toC(s.x, s.y, mdnC, b.xMin, b.xMax, b.yMin, b.yMax);
            mdnCtx.beginPath(); mdnCtx.arc(cx, cy, 4.5, 0, Math.PI * 2); mdnCtx.fill();
        });
    }
}

function drawComp(xPos) {
    clear(compCtx, compC);
    const pad = { l: 60, r: 30, t: 30, b: 40 };
    const w = compC.width - pad.l - pad.r, h = compC.height - pad.t - pad.b;
    compCtx.strokeStyle = '#e5e3df'; compCtx.lineWidth = 1;
    compCtx.beginPath(); compCtx.moveTo(pad.l, compC.height - pad.b); compCtx.lineTo(compC.width - pad.r, compC.height - pad.b); compCtx.stroke();
    compCtx.beginPath(); compCtx.moveTo(pad.l, pad.t); compCtx.lineTo(pad.l, compC.height - pad.b); compCtx.stroke();
    compCtx.fillStyle = '#999'; compCtx.font = '11px "JetBrains Mono", monospace';
    compCtx.fillText('x', compC.width - pad.r + 8, compC.height - pad.b + 4);
    compCtx.fillText('p(x|y)', pad.l - 10, pad.t - 10);
    
    const b = inverseBounds;
    const actualX = b.xMin + xPos * (b.xMax - b.xMin);
    const p = mdn.getParams(actualX);
    const K = mdn.K;
    const ys = [], mix = [], comps = [[], [], []];
    let maxD = 0.001;
    for (let i = 0; i <= 100; i++) {
        const y = b.yMin + (i / 100) * (b.yMax - b.yMin);
        ys.push(i / 100);
        let m = 0;
        for (let k = 0; k < K; k++) { const d = p.pi[k] * mdn.gauss(y, p.mu[k], p.sigma[k]); comps[k].push(d); m += d; }
        mix.push(m); maxD = Math.max(maxD, m);
    }
    
    const cols = ['rgba(74,144,226,0.35)', 'rgba(80,200,120,0.35)', 'rgba(245,166,35,0.35)'];
    const solid = ['#4a90e2', '#50c878', '#f5a623'];
    
    for (let k = 0; k < K; k++) {
        compCtx.fillStyle = cols[k]; compCtx.beginPath();
        compCtx.moveTo(pad.l, compC.height - pad.b);
        for (let i = 0; i < ys.length; i++) {
            const px = pad.l + ys[i] * w, py = compC.height - pad.b - (comps[k][i] / maxD) * h * 0.9;
            compCtx.lineTo(px, py);
        }
        compCtx.lineTo(compC.width - pad.r, compC.height - pad.b);
        compCtx.closePath(); compCtx.fill();
    }
    compCtx.strokeStyle = '#c45d4a'; compCtx.lineWidth = 2.5; compCtx.beginPath();
    for (let i = 0; i < ys.length; i++) {
        const px = pad.l + ys[i] * w, py = compC.height - pad.b - (mix[i] / maxD) * h * 0.9;
        i === 0 ? compCtx.moveTo(px, py) : compCtx.lineTo(px, py);
    }
    compCtx.stroke();
    
    compCtx.font = '10px "JetBrains Mono", monospace';
    for (let k = 0; k < K; k++) {
        const lx = pad.l + 5 + k * 180;
        compCtx.fillStyle = solid[k];
        compCtx.fillRect(lx, pad.t - 5, 10, 10);
        compCtx.fillStyle = '#666';
        compCtx.fillText(`π${k+1}=${p.pi[k].toFixed(2)} μ=${p.mu[k].toFixed(2)}`, lx + 14, pad.t + 4);
    }
}

// ================================================================
// TRAINING
// ================================================================
// Helper to compute mean and std
function computeStats(data, key) {
    let sum = 0, sum2 = 0;
    data.forEach(p => { sum += p[key]; sum2 += p[key] * p[key]; });
    const mean = sum / data.length;
    const std = Math.sqrt(sum2 / data.length - mean * mean) || 1;
    return { mean, std };
}

document.getElementById('trainForward').addEventListener('click', async function() {
    this.disabled = true;
    const { X, Y } = toMat(forwardData);
    
    // Compute normalization stats
    const xStats = computeStats(forwardData, 'x');
    const yStats = computeStats(forwardData, 'y');
    forwardMLP.setNorm(xStats.mean, xStats.std, yStats.mean, yStats.std);
    
    const epochs = 500, lr = 0.1;
    document.getElementById('forwardStatus').textContent = 'Training...';
    for (let e = 0; e < epochs; e++) {
        forwardMLP.forward(X); forwardMLP.backward(Y, lr);
        if (e % 10 === 0 || e === epochs - 1) {
            forwardTrained = true;
            const loss = forwardMLP.loss(Y);
            document.getElementById('forwardEpoch').textContent = e + 1;
            document.getElementById('forwardLoss').textContent = loss.toFixed(4);
            document.getElementById('forwardProgress').style.width = ((e + 1) / epochs * 100) + '%';
            drawForward();
            await new Promise(r => setTimeout(r, 0));
        }
    }
    document.getElementById('forwardStatus').textContent = 'Done ✓';
    this.disabled = false;
});

document.getElementById('resetForward').addEventListener('click', () => {
    forwardData = genForwardData(500);
    inverseData = invertData(forwardData);
    forwardBounds = getBounds(forwardData);
    inverseBounds = getBounds(inverseData);
    forwardMLP = new MLP(1, 32, 1);
    inverseMLP = new MLP(1, 64, 1);
    mdn = new MDN(1, 64, 3);
    forwardTrained = false; inverseTrained = false; mdnTrained = false;
    mdnSamples = [];
    drawForward(); drawInverse(); drawMDN(); drawComp(0.5);
    ['forward', 'inverse', 'mdn'].forEach(id => {
        document.getElementById(id + 'Epoch').textContent = '0';
        document.getElementById(id + 'Loss').textContent = '-';
        document.getElementById(id + 'Status').textContent = 'Ready';
        document.getElementById(id + 'Progress').style.width = '0%';
    });
});

document.getElementById('trainInverse').addEventListener('click', async function() {
    this.disabled = true;
    const { X, Y } = toMat(inverseData);
    
    // Compute normalization stats for inverse data
    const xStats = computeStats(inverseData, 'x');
    const yStats = computeStats(inverseData, 'y');
    inverseMLP.setNorm(xStats.mean, xStats.std, yStats.mean, yStats.std);
    
    const epochs = 800, lr = 0.05;
    document.getElementById('inverseStatus').textContent = 'Training...';
    for (let e = 0; e < epochs; e++) {
        inverseMLP.forward(X); inverseMLP.backward(Y, lr);
        if (e % 16 === 0 || e === epochs - 1) {
            inverseTrained = true;
            const loss = inverseMLP.loss(Y);
            document.getElementById('inverseEpoch').textContent = e + 1;
            document.getElementById('inverseLoss').textContent = loss.toFixed(4);
            document.getElementById('inverseProgress').style.width = ((e + 1) / epochs * 100) + '%';
            drawInverse();
            await new Promise(r => setTimeout(r, 0));
        }
    }
    document.getElementById('inverseStatus').textContent = 'Done';
    this.disabled = false;
});

document.getElementById('resetInverse').addEventListener('click', () => {
    inverseMLP = new MLP(1, 64, 1);
    inverseTrained = false;
    drawInverse();
    document.getElementById('inverseEpoch').textContent = '0';
    document.getElementById('inverseLoss').textContent = '-';
    document.getElementById('inverseStatus').textContent = 'Ready';
    document.getElementById('inverseProgress').style.width = '0%';
});

document.getElementById('trainMDN').addEventListener('click', async function() {
    this.disabled = true;
    const { X, Y } = toMat(inverseData);
    
    // Compute normalization stats for inverse data
    const xStats = computeStats(inverseData, 'x');
    const yStats = computeStats(inverseData, 'y');
    mdn.setNorm(xStats.mean, xStats.std, yStats.mean, yStats.std);
    
    const epochs = 1200, lr = 0.08;
    document.getElementById('mdnStatus').textContent = 'Training...';
    for (let e = 0; e < epochs; e++) {
        mdn.forward(X); mdn.backward(Y, lr);
        if (e % 24 === 0 || e === epochs - 1) {
            mdnTrained = true;
            const loss = mdn.loss(Y);
            document.getElementById('mdnEpoch').textContent = e + 1;
            document.getElementById('mdnLoss').textContent = isNaN(loss) ? 'NaN' : loss.toFixed(4);
            document.getElementById('mdnProgress').style.width = ((e + 1) / epochs * 100) + '%';
            drawMDN();
            drawComp(parseFloat(document.getElementById('xSlider').value) / 100);
            await new Promise(r => setTimeout(r, 0));
        }
    }
    document.getElementById('mdnStatus').textContent = 'Done ✓';
    this.disabled = false;
});

document.getElementById('sampleMDN').addEventListener('click', () => {
    if (!mdnTrained) return;
    const b = inverseBounds;
    mdnSamples = [];
    for (let i = 0; i < 80; i++) {
        const x = b.xMin + 0.05 * (b.xMax - b.xMin) + Math.random() * 0.9 * (b.xMax - b.xMin);
        const y = mdn.sample(x);
        if (y > b.yMin && y < b.yMax) mdnSamples.push({ x, y });
    }
    drawMDN();
});

document.getElementById('resetMDN').addEventListener('click', () => {
    mdn = new MDN(1, 64, 3);
    mdnTrained = false; mdnSamples = [];
    drawMDN();
    drawComp(parseFloat(document.getElementById('xSlider').value) / 100);
    document.getElementById('mdnEpoch').textContent = '0';
    document.getElementById('mdnLoss').textContent = '-';
    document.getElementById('mdnStatus').textContent = 'Ready';
    document.getElementById('mdnProgress').style.width = '0%';
});

document.getElementById('xSlider').addEventListener('input', function() {
    const x = parseFloat(this.value) / 100;
    document.getElementById('xValue').textContent = `y = ${x.toFixed(2)}`;
    drawComp(x);
});

// Init
drawForward(); drawInverse(); drawMDN(); drawComp(0.5);
</script>
</body>
</html>